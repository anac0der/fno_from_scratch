{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Neural Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralop\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "\n",
    "torch.manual_seed(2002)\n",
    "np.random.seed(2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "В качестве входных данных сеть получает начальное условие для уравнения Бюргерса $$ {\\frac {\\partial u}{\\partial t}}+u{\\frac {\\partial u}{\\partial x}}=\\nu {\\frac {\\partial ^{2}u}{\\partial x^{2}}}$$ на интервале $[0, length / 2]$ и экстраполирует его на интервал $[length / 2, length]$, где length=256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап 1: обучение FNO из пакета neuralop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом этапе задача будет решаться с официальной реализацией FNO из пакета neuralop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путь к данным. Лучше поменять, так как я прописывал его относительно базовой директории окружения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"./Task2_with_datasets/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.burgers import load_burgers_1dtime\n",
    "from dataloaders.tensor_dataset import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При загрузке данным надо вытащить сами массивы данных, чтобы разбить их на x и y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, train, test = load_burgers_1dtime(\n",
    "    data_path=os.path.join(datasets_path, \"burgers\"),\n",
    "    n_train=400, n_test=100, batch_size=16, batch_size_test=16,\n",
    "    temporal_length=1, spatial_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3, 1, 256]) torch.Size([100, 3, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3, 256]) torch.Size([100, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "train = torch.squeeze(train, dim=2)\n",
    "test = torch.squeeze(test, dim=2)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[:, :, :128]\n",
    "y_train =  torch.unsqueeze(train[:, 0, 128:], dim=1)\n",
    "\n",
    "x_test = test[:, :, :128]\n",
    "y_test = torch.unsqueeze(test[:, 0, 128:], dim=1)\n",
    "train_db = TensorDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_db, batch_size=32, shuffle=False)\n",
    "\n",
    "test_db = TensorDataset(x_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_db, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralop.models import FNO\n",
    "from neuralop.utils import count_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим 1-d FNO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 205249 parameters.\n",
      "\n",
      "### MODEL ###\n",
      " FNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): FactorizedSpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-5): 6 x ComplexDenseTensor(shape=torch.Size([32, 32, 16]), rank=None)\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-5): 6 x Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (lifting): Lifting(\n",
      "    (fc): Conv1d(3, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (projection): Projection(\n",
      "    (fc1): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "    (fc2): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.StepLR object at 0x7feb98020df0>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.training.losses.H1Loss object at 0x7fecb26f59a0>\n",
      "\n",
      " * Test: {'h1': <neuralop.training.losses.H1Loss object at 0x7fecb26f59a0>, 'l2': <neuralop.training.losses.LpLoss object at 0x7fed7478ee20>}\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = FNO(n_modes=(32,), in_channels=3, out_channels=1, n_layers=6, hidden_channels=32, projection_channels=64)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100000, gamma=1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=5e-4)\n",
    "\n",
    "h1loss = H1Loss(d=1, reductions='mean')\n",
    "l2loss =LpLoss(d=1, p=2, reductions='mean')\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}\n",
    "\n",
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем Trainer с коллбэком для Tensorboard и обучаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks.tensorboard_callback import TensorBoardCallback\n",
    "from callbacks.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard method to load data to device.\n",
      "using standard method to compute loss.\n",
      "Training on regular inputs (no multi-grid patching).\n",
      "Training on 400 samples\n",
      "Testing on [100] samples         on resolutions ['test'].\n",
      "Training on raw inputs of size x.shape=torch.Size([32, 3, 128]), y.shape=torch.Size([32, 1, 128])\n",
      ".. patched inputs of size x.shape=torch.Size([32, 3, 128]), y.shape=torch.Size([32, 1, 128])\n",
      "Raw outputs of size out.shape=torch.Size([32, 1, 128])\n",
      ".. Processed (unpatched) outputs of size out.shape=torch.Size([32, 1, 128])\n",
      "[0] time=0.12, avg_loss=0.0647, train_err=0.0327, test_h1=0.0702, test_l2=0.0699\n",
      "[3] time=0.12, avg_loss=0.0577, train_err=0.0291, test_h1=0.0588, test_l2=0.0591\n",
      "[6] time=0.12, avg_loss=0.0527, train_err=0.0266, test_h1=0.0525, test_l2=0.0580\n",
      "[9] time=0.12, avg_loss=0.0499, train_err=0.0252, test_h1=0.0497, test_l2=0.0569\n",
      "[12] time=0.12, avg_loss=0.0426, train_err=0.0215, test_h1=0.0408, test_l2=0.0421\n",
      "[15] time=0.12, avg_loss=0.0395, train_err=0.0200, test_h1=0.0407, test_l2=0.0447\n",
      "[18] time=0.12, avg_loss=0.0390, train_err=0.0197, test_h1=0.0402, test_l2=0.0435\n",
      "[21] time=0.12, avg_loss=0.0386, train_err=0.0195, test_h1=0.0389, test_l2=0.0398\n",
      "[24] time=0.12, avg_loss=0.0387, train_err=0.0195, test_h1=0.0396, test_l2=0.0383\n",
      "[27] time=0.12, avg_loss=0.0384, train_err=0.0194, test_h1=0.0395, test_l2=0.0383\n",
      "[30] time=0.12, avg_loss=0.0381, train_err=0.0192, test_h1=0.0392, test_l2=0.0382\n",
      "[33] time=0.12, avg_loss=0.0380, train_err=0.0192, test_h1=0.0392, test_l2=0.0381\n",
      "[36] time=0.12, avg_loss=0.0379, train_err=0.0191, test_h1=0.0392, test_l2=0.0380\n",
      "[39] time=0.12, avg_loss=0.0378, train_err=0.0191, test_h1=0.0391, test_l2=0.0379\n",
      "[42] time=0.12, avg_loss=0.0377, train_err=0.0190, test_h1=0.0392, test_l2=0.0378\n",
      "[45] time=0.12, avg_loss=0.0376, train_err=0.0190, test_h1=0.0392, test_l2=0.0377\n",
      "[48] time=0.12, avg_loss=0.0375, train_err=0.0189, test_h1=0.0393, test_l2=0.0376\n",
      "[51] time=0.12, avg_loss=0.0374, train_err=0.0189, test_h1=0.0393, test_l2=0.0376\n",
      "[54] time=0.12, avg_loss=0.0373, train_err=0.0188, test_h1=0.0393, test_l2=0.0375\n",
      "[57] time=0.12, avg_loss=0.0372, train_err=0.0188, test_h1=0.0394, test_l2=0.0375\n",
      "[60] time=0.12, avg_loss=0.0371, train_err=0.0187, test_h1=0.0394, test_l2=0.0375\n",
      "[63] time=0.12, avg_loss=0.0369, train_err=0.0187, test_h1=0.0394, test_l2=0.0375\n",
      "[66] time=0.12, avg_loss=0.0368, train_err=0.0186, test_h1=0.0394, test_l2=0.0374\n",
      "[69] time=0.12, avg_loss=0.0366, train_err=0.0185, test_h1=0.0393, test_l2=0.0374\n",
      "[72] time=0.12, avg_loss=0.0365, train_err=0.0184, test_h1=0.0392, test_l2=0.0374\n",
      "[75] time=0.12, avg_loss=0.0364, train_err=0.0184, test_h1=0.0392, test_l2=0.0374\n",
      "[78] time=0.12, avg_loss=0.0362, train_err=0.0183, test_h1=0.0392, test_l2=0.0374\n",
      "[81] time=0.12, avg_loss=0.0361, train_err=0.0182, test_h1=0.0392, test_l2=0.0373\n",
      "[84] time=0.12, avg_loss=0.0360, train_err=0.0182, test_h1=0.0392, test_l2=0.0373\n",
      "[87] time=0.12, avg_loss=0.0358, train_err=0.0181, test_h1=0.0391, test_l2=0.0372\n",
      "[90] time=0.12, avg_loss=0.0357, train_err=0.0180, test_h1=0.0391, test_l2=0.0371\n",
      "[93] time=0.12, avg_loss=0.0356, train_err=0.0180, test_h1=0.0390, test_l2=0.0370\n",
      "[96] time=0.12, avg_loss=0.0355, train_err=0.0179, test_h1=0.0390, test_l2=0.0370\n",
      "[99] time=0.12, avg_loss=0.0354, train_err=0.0179, test_h1=0.0389, test_l2=0.0370\n",
      "[102] time=0.12, avg_loss=0.0352, train_err=0.0178, test_h1=0.0389, test_l2=0.0370\n",
      "[105] time=0.12, avg_loss=0.0351, train_err=0.0177, test_h1=0.0388, test_l2=0.0370\n",
      "[108] time=0.12, avg_loss=0.0351, train_err=0.0177, test_h1=0.0389, test_l2=0.0370\n",
      "[111] time=0.12, avg_loss=0.0348, train_err=0.0176, test_h1=0.0390, test_l2=0.0371\n",
      "[114] time=0.12, avg_loss=0.0346, train_err=0.0175, test_h1=0.0388, test_l2=0.0373\n",
      "[117] time=0.12, avg_loss=0.0344, train_err=0.0174, test_h1=0.0386, test_l2=0.0373\n",
      "[120] time=0.12, avg_loss=0.0343, train_err=0.0173, test_h1=0.0387, test_l2=0.0374\n",
      "[123] time=0.12, avg_loss=0.0342, train_err=0.0172, test_h1=0.0388, test_l2=0.0375\n",
      "[126] time=0.12, avg_loss=0.0340, train_err=0.0172, test_h1=0.0388, test_l2=0.0376\n",
      "[129] time=0.12, avg_loss=0.0339, train_err=0.0171, test_h1=0.0387, test_l2=0.0376\n",
      "[132] time=0.12, avg_loss=0.0338, train_err=0.0171, test_h1=0.0387, test_l2=0.0376\n",
      "[135] time=0.12, avg_loss=0.0336, train_err=0.0170, test_h1=0.0387, test_l2=0.0376\n",
      "[138] time=0.12, avg_loss=0.0336, train_err=0.0169, test_h1=0.0386, test_l2=0.0375\n",
      "[141] time=0.12, avg_loss=0.0334, train_err=0.0169, test_h1=0.0386, test_l2=0.0376\n",
      "[144] time=0.12, avg_loss=0.0333, train_err=0.0168, test_h1=0.0386, test_l2=0.0374\n",
      "[147] time=0.12, avg_loss=0.0332, train_err=0.0168, test_h1=0.0385, test_l2=0.0376\n",
      "[150] time=0.12, avg_loss=0.0331, train_err=0.0167, test_h1=0.0385, test_l2=0.0372\n",
      "[153] time=0.12, avg_loss=0.0331, train_err=0.0167, test_h1=0.0385, test_l2=0.0371\n",
      "[156] time=0.12, avg_loss=0.0329, train_err=0.0166, test_h1=0.0385, test_l2=0.0373\n",
      "[159] time=0.12, avg_loss=0.0328, train_err=0.0165, test_h1=0.0385, test_l2=0.0367\n",
      "[162] time=0.12, avg_loss=0.0327, train_err=0.0165, test_h1=0.0386, test_l2=0.0368\n",
      "[165] time=0.12, avg_loss=0.0326, train_err=0.0165, test_h1=0.0386, test_l2=0.0369\n",
      "[168] time=0.12, avg_loss=0.0325, train_err=0.0164, test_h1=0.0386, test_l2=0.0367\n",
      "[171] time=0.12, avg_loss=0.0324, train_err=0.0164, test_h1=0.0386, test_l2=0.0366\n",
      "[174] time=0.12, avg_loss=0.0323, train_err=0.0163, test_h1=0.0386, test_l2=0.0366\n",
      "[177] time=0.12, avg_loss=0.0323, train_err=0.0163, test_h1=0.0386, test_l2=0.0363\n",
      "[180] time=0.12, avg_loss=0.0324, train_err=0.0164, test_h1=0.0385, test_l2=0.0363\n",
      "[183] time=0.12, avg_loss=0.0326, train_err=0.0165, test_h1=0.0385, test_l2=0.0361\n",
      "[186] time=0.12, avg_loss=0.0322, train_err=0.0163, test_h1=0.0390, test_l2=0.0369\n",
      "[189] time=0.12, avg_loss=0.0326, train_err=0.0165, test_h1=0.0387, test_l2=0.0370\n",
      "[192] time=0.12, avg_loss=0.0320, train_err=0.0162, test_h1=0.0384, test_l2=0.0361\n",
      "[195] time=0.12, avg_loss=0.0318, train_err=0.0161, test_h1=0.0385, test_l2=0.0364\n",
      "[198] time=0.12, avg_loss=0.0317, train_err=0.0160, test_h1=0.0385, test_l2=0.0363\n",
      "[201] time=0.12, avg_loss=0.0315, train_err=0.0159, test_h1=0.0386, test_l2=0.0365\n"
     ]
    }
   ],
   "source": [
    "tb_callback = TensorBoardCallback(log_dir=f'./logs/run{exp_num}')\n",
    "trainer = Trainer(model=model, n_epochs=202,\n",
    "                  device=device,\n",
    "                  wandb_log=False,\n",
    "                  log_test_interval=3,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True,\n",
    "                  callbacks=[tb_callback])\n",
    "\n",
    "trainer.train(train_loader=train_loader, model=model, \n",
    "              output_encoder=None,\n",
    "              test_loaders=test_loader,\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler,\n",
    "              regularizer=False,\n",
    "              training_loss=train_loss,\n",
    "              eval_losses=eval_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из результатов экспериментов -- MSE на тестовой выборке после послежней эпохи в районе $4 \\cdot 10^{-2}$. Кажется, это неплохой результат.\n",
    "\n",
    "Посмотрим на результаты экспериментов в Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-01 13:00:15.540470: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-01 13:00:15.964079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-01 13:00:16.514837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-01 13:00:16.515284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-01 13:00:16.519499: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "I1201 13:00:16.878144 139782943430400 plugin.py:429] Monitor runs begin\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
